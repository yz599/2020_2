{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "(a)  \n",
    "the total count is 21, while +Y is 12 and -Y has 9. Therefore, P(Y=+) = 12/21 = 4/7, P(Y=-) = 3/7  \n",
    "The entropy is H(4/7,3/7) = -4/7*log2(3/7)-3/7*log2(4/7) = 0.9852\n",
    "(b)  \n",
    "- variable x1  \n",
    "T: p1 = 7, n1 = 1  \n",
    "F： p2 = 5, n2 = 8\n",
    "p = 12, n=9\n",
    "E(H(x1)) = (7+1)/(12+9)*{-7/8*log2(1/8)-1/8*log2(7/8)}+   \n",
    "(5+8)/(12+9)*{-5/13*log2(8/13)-8/13*log2(5/13)} = 0.8021    \n",
    "IG(x1) = H(4/7,3/7)-E(H(x1))= 0.1831  \n",
    "- variable x2  \n",
    "T: p1 = 7, n1 = 3  \n",
    "F： p2 = 5, n2 = 6\n",
    "p = 12, n=9\n",
    "E(H(x2)) = (7+3)/(12+9)*{-7/10*log2(3/10)-3/10*log2(7/10)} +   \n",
    "(5+6)/(12+9)*{-5/11*log2(6/11)-6/11*log2(5/11)} = 0.9403     \n",
    "IG(x2) = H(4/7,3/7)-E(H(x4))=  0.0449    \n",
    "\n",
    "So we should priortize variable x1  \n",
    "\n",
    "(c)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def HY(p,n):\n",
    "    if p*n:\n",
    "         return -p/(p+n)*math.log2(p/(p+n))-n/(p+n)*math.log2(n/(p+n))\n",
    "    else:\n",
    "         return 0\n",
    "\n",
    "def EH(ps,ns):\n",
    "    EH=0\n",
    "    for i,p in enumerate(ps):\n",
    "        EH += (p+ns[i])/(sum(ps)+sum(ns))*HY(p,ns[i])\n",
    "    return EH\n",
    "\n",
    "def IG(ps,ns):\n",
    "    return HY(sum(ps),sum(ns)) - EH(ps,ns)\n",
    "\n",
    "print(HY(3/7,4/7))\n",
    "print(EH([7,5],[1,8]))\n",
    "print(IG([7,5],[1,8]))\n",
    "print(EH([7,5],[3,6]))\n",
    "print(IG([7,5],[3,6]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. \n",
    "\n",
    "(a)\n",
    "P(A=Yes)=0.6, P(A=No)=0.4  \n",
    "(b）\n",
    "\n",
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = [[216,5.68],[69,4.78],[302,2.31],[60,3.16],[393,4.2]]\n",
    "mean = np.mean(data,axis=0)\n",
    "std = np.std(data,axis=0,ddof=1)\n",
    "data_S = (data-mean)/std\n",
    "data_S_A_p  = data_S[[0,1,3]]\n",
    "data_S_A_n  = data_S[[2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for A\n",
    "mean_p = np.mean(data_S_A_p,axis=0)\n",
    "std_p = np.std(data_S_A_p,axis=0)\n",
    "print(mean_p)\n",
    "print(std_p)\n",
    "\n",
    "# model for not A\n",
    "mean_n = np.mean(data_S_A_n,axis=0)\n",
    "std_n = np.std(data_S_A_n,axis=0)\n",
    "print(mean_n)\n",
    "print(std_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = iris.data[:,:2]\n",
    "y = (iris.target!=0)*1\n",
    "mean = np.mean(X,axis=0)\n",
    "std = np.std(X,axis=0,ddof=1)\n",
    "X_S = (X-mean)/std # standardize the data\n",
    "\n",
    "# prepare the data\n",
    "X_S = np.insert(X_S,0,np.ones(len(X_S)),axis=1) # add bias term\n",
    "X_S_T = np.transpose(X_S)\n",
    "\n",
    "\n",
    "# Gradient Descent algorithm\n",
    "iter_num = 1\n",
    "loss_diff = 1\n",
    "Losses = []\n",
    "theta = 2*(np.random.rand(np.shape(X_S)[1])-0.5) # initialize with a aryay in [-1 1]\n",
    "\n",
    "while iter_num <10000 and loss_diff>2e-23:\n",
    "    # update theta\n",
    "    theta = theta + 0.01/len(X_S_T)*X_S_T.dot(y-1/(1+np.exp(-X_S.dot(theta))))\n",
    "    \n",
    "    loss=0\n",
    "    for ii,Yt in enumerate(y):\n",
    "        g_x_theta = 1/(1+np.exp(-X_S[ii].dot(theta)))\n",
    "\n",
    "        if Yt==1:  # To avoid Runtimewarning in log function\n",
    "            loss +=  Yt*np.log(g_x_theta)\n",
    "        else:\n",
    "            loss +=  Yt*np.log(g_x_theta)+(1-Yt)*np.log(1-g_x_theta)\n",
    "        \n",
    "    if iter_num==1:\n",
    "        loss_diff = abs(loss)\n",
    "    else:\n",
    "        loss_diff = abs((loss - Losses[-1])/loss)\n",
    "\n",
    "    Losses.append(loss)\n",
    "    iter_num += 1\n",
    "\n",
    "print('The final model  theta values is: '+str(theta))\n",
    "print('The final loss value is: '+str(loss))\n",
    "\n",
    "#     # apply to the testing data and calculate SE\n",
    "#     y_predicted = X_S.dot(theta)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1,iter_num), Losses)\n",
    "plt.xlabel('# of iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "# plt.savefig('images\\Q5.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "idx1 = y==1\n",
    "idx2 = y==0\n",
    "\n",
    "plt.scatter(X_S[idx1,1], X_S[idx1,2], color = 'green', label = \"Iris setosa\")\n",
    "plt.scatter(X_S[idx2,1], X_S[idx2,2], color = 'blue', label = \"Iris  virgincia  and  versicolor\")\n",
    "\n",
    "# getting the x co-ordinates of the decision boundary\n",
    "plot_x = np.array([min(X_S[:,1]) - 0.2, max(X_S[:,1]) + 0.2])\n",
    "# getting corresponding y co-ordinates of the decision boundary\n",
    "plot_y = (-1/theta[2]) * (theta[1] * plot_x + theta[0])\n",
    "plt.plot(plot_x, plot_y, label = \"Decision_Boundary\")\n",
    "plt.legend()\n",
    "plt.xlabel('Width of sepals - normalized')\n",
    "plt.ylabel('Length of sepals - normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(X_S[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with sklearn method    \n",
    "from  sklearn.linear_model  import LogisticRegression\n",
    "lgr  =  LogisticRegression( penalty ='none' , solver ='lbfgs', max_iter =10000)\n",
    "lgr.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = lgr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k',  cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal width')\n",
    "plt.ylabel('Sepal length')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Logistic Regression Spam Classi\f",
    "cation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spambase(1).data',header=None)\n",
    "data = df.values\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# prepare the data\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data[:,0:-2],data[:,-1],train_size = 2/3)\n",
    "\n",
    "# standardize the train data and testing data\n",
    "mean = np.mean(x_train,axis=0)\n",
    "std = np.std(x_train,axis=0,ddof=1)\n",
    "x_train_S = (x_train-mean)/std\n",
    "x_train_S = np.insert(x_train_S,0,np.ones(len(x_train_S)),axis=1) # add bias term\n",
    "x_train_S_T = np.transpose(x_train_S)\n",
    "x_test_S = (x_test-mean)/std\n",
    "x_test_S = np.insert(x_test_S,0,np.ones(len(x_test_S)),axis=1) # add bias term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent algorithm\n",
    "iter_num = 1\n",
    "loss_diff = 1\n",
    "Losses = []\n",
    "theta = 2*(np.random.rand(np.shape(x_train_S)[1])-0.5) # initialize with a aryay in [-1 1]\n",
    "\n",
    "while iter_num <10000 and loss_diff>2e-23:\n",
    "    # update theta\n",
    "    theta = theta + 0.01/len(x_train_S)*x_train_S_T.dot(y_train-1/(1+np.exp(-x_train_S.dot(theta))))\n",
    "    \n",
    "    loss=0\n",
    "    for ii,Yt in enumerate(y_train):\n",
    "        g_x_theta = 1/(1+np.exp(-x_train_S[ii].dot(theta)))\n",
    "\n",
    "        if Yt==1:  # To avoid Runtimewarning in log function\n",
    "            loss +=  Yt*np.log(g_x_theta)\n",
    "        else:\n",
    "            loss +=  Yt*np.log(g_x_theta)+(1-Yt)*np.log(1-g_x_theta)\n",
    "        \n",
    "    if iter_num==1:\n",
    "        loss_diff = abs(loss)\n",
    "    else:\n",
    "        loss_diff = abs((loss - Losses[-1])/loss)\n",
    "\n",
    "    Losses.append(loss)\n",
    "    iter_num += 1\n",
    "\n",
    "print('The final model  theta values is: '+str(theta))\n",
    "print('The final loss value is: '+str(loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(range(1,iter_num), Losses)\n",
    "plt.xlabel('# of iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "# plt.savefig('images\\Q5.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the model with the testing data\n",
    "y_test_predicted = x_test_S.dot(theta)\n",
    "TP=0\n",
    "FN=0\n",
    "FP=0\n",
    "TN=0\n",
    "for ii,y_p in enumerate(y_test_predicted):   \n",
    "    if y_p>=0.5:  # choose the lable based on threshold 0.5\n",
    "        y_p=1\n",
    "    else:\n",
    "        y_p=0\n",
    "    \n",
    "    y_t = y_test[ii]   # the real y value from the test data\n",
    "    if y_t==1 and y_p==1:\n",
    "        TP +=1\n",
    "    elif y_t==1 and y_p==0:\n",
    "        FN +=1\n",
    "    elif y_t==0 and y_p==1:\n",
    "        FP +=1\n",
    "    else:\n",
    "        TN +=1\n",
    "        \n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F_measure = 2*precision*recall/(precision+recall)\n",
    "accuracy = (TP+TN)/len(y_test)\n",
    "print(\"The precision is: \"+str(precision))\n",
    "print(\"The recall is: \"+str(recall))\n",
    "print(\"The F_measure is: \"+str(F_measure))\n",
    "print(\"The accuracy is: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('spambase(1).data',header=None)\n",
    "data = df.values\n",
    "\n",
    "# prepare the data\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data[:,0:-2],data[:,-1],train_size = 2/3)\n",
    "\n",
    "# standardize the train data and testing data\n",
    "mean = np.mean(x_train,axis=0)\n",
    "std = np.std(x_train,axis=0,ddof=1)\n",
    "x_train_S = (x_train-mean)/std\n",
    "x_test_S = (x_test-mean)/std\n",
    "\n",
    "# divide the data into spam exmaples and non-spam examples\n",
    "idx_spam = y_train==1\n",
    "idx_no_spam = y_train==0\n",
    "\n",
    "x_train_S_spam = x_train_S[idx_spam]\n",
    "x_train_S_no_spam = x_train_S[idx_no_spam]\n",
    "\n",
    "# calculate the mean and std values for both spam and non-spam examples\n",
    "mean_spam = np.mean(x_train_S_spam,axis=0)\n",
    "std_spam = np.std(x_train_S_spam,axis=0,ddof=1)\n",
    "\n",
    "mean_no_spam = np.mean(x_train_S_no_spam,axis=0)\n",
    "std_no_spam = np.std(x_train_S_no_spam,axis=0,ddof=1)\n",
    "\n",
    "# Classify testing samples and compute statistics using the testing data results\n",
    "TP=0\n",
    "FN=0\n",
    "FP=0\n",
    "TN=0\n",
    "for ii,y_t in enumerate(y_test):   \n",
    "    x_t = x_test_S[ii] \n",
    "    \n",
    "    # calaulate the probability of testing samples and chosse labels\n",
    "    P_spam = np.log(len(x_train_S_spam)/len(x_train_S))\n",
    "    P_no_spam = np.log(len(x_train_S_no_spam)/len(x_train_S))\n",
    "    for k,x_t_k in enumerate(x_t):\n",
    "        P_spam += np.log(1/std_spam[k]) + (-((x_t_k-mean_spam[k])/std_spam[k])**2/2)\n",
    "        P_no_spam += np.log(1/std_no_spam[k])+ (-((x_t_k-mean_no_spam[k])/std_no_spam[k])**2/2)\n",
    "    \n",
    "#     print(P_spam)\n",
    "#     print(P_no_spam)\n",
    "    # choose the lable based on higher probability of each class\n",
    "    if P_spam>=P_no_spam:  \n",
    "        y_p=1\n",
    "    else:\n",
    "        y_p=0\n",
    "    \n",
    "    # Compate the statistics\n",
    "    if y_t==1 and y_p==1:\n",
    "        TP +=1\n",
    "    elif y_t==1 and y_p==0:\n",
    "        FN +=1\n",
    "    elif y_t==0 and y_p==1:\n",
    "        FP +=1\n",
    "    else:\n",
    "        TN +=1\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F_measure = 2*precision*recall/(precision+recall)\n",
    "accuracy = (TP+TN)/len(y_test)\n",
    "print(\"The precision is: \"+str(precision))\n",
    "print(\"The recall is: \"+str(recall))\n",
    "print(\"The F_measure is: \"+str(F_measure))\n",
    "print(\"The accuracy is: \"+str(accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4596</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4597</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4598</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4599</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3     4     5     6     7     8     9   ...     48  \\\n",
       "0     0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "1     0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.000   \n",
       "2     0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.010   \n",
       "3     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "4     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "...    ...   ...   ...  ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "4596  0.31  0.00  0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4597  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4598  0.30  0.00  0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.102   \n",
       "4599  0.96  0.00  0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4600  0.00  0.00  0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "\n",
       "         49   50     51     52     53     54   55    56  57  \n",
       "0     0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1     0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2     0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3     0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4     0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "...     ...  ...    ...    ...    ...    ...  ...   ...  ..  \n",
       "4596  0.232  0.0  0.000  0.000  0.000  1.142    3    88   0  \n",
       "4597  0.000  0.0  0.353  0.000  0.000  1.555    4    14   0  \n",
       "4598  0.718  0.0  0.000  0.000  0.000  1.404    6   118   0  \n",
       "4599  0.057  0.0  0.000  0.000  0.000  1.147    5    78   0  \n",
       "4600  0.000  0.0  0.125  0.000  0.000  1.250    5    40   0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spambase(1).data',header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_prep(df):\n",
    "    features=df[df.columns[0:-1]]\n",
    "    labels=df[df.columns[-1]]\n",
    "    \n",
    "    # standardize the train data and testing data\n",
    "    mean = features.mean()\n",
    "    std = features.std()\n",
    "    features_Nor = (features-mean)/std\n",
    "\n",
    "    # Discretize the attribute values\n",
    "    features_Nor_cat=pd.DataFrame()\n",
    "    features_Nor_cat=features_Nor.apply(lambda x:pd.cut(x, bins=[-np.inf,x.mean(),np.inf],labels=[0,1]),axis=1)\n",
    "    \n",
    "    dataset = pd.concat([features_Nor_cat,labels], axis=1, sort=False)\n",
    "    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calucuate entropy\n",
    "def entropy(labels):\n",
    "    prob=labels.value_counts(normalize=True)\n",
    "    entropy=(-prob*np.log2(prob)).sum()\n",
    "    \n",
    "    return entropy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4596</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4597</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4598</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4599</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9   ...  48  49  50  51  52  53  54  \\\n",
       "0      0   1   1   1   1   0   0   0   0   0  ...   0   0   0   1   0   0   1   \n",
       "1      1   0   1   0   0   1   1   0   0   1  ...   0   0   0   1   1   0   0   \n",
       "2      0   0   1   0   1   1   0   0   1   0  ...   0   0   0   0   1   0   0   \n",
       "3      0   0   0   1   1   0   1   1   1   1  ...   0   1   0   0   0   0   1   \n",
       "4      0   0   0   1   1   0   1   1   1   1  ...   0   1   0   0   0   0   1   \n",
       "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..   \n",
       "4596   1   0   1   1   0   1   0   0   0   0  ...   0   1   0   0   0   1   0   \n",
       "4597   0   0   0   1   0   0   0   0   0   0  ...   0   0   0   1   0   1   1   \n",
       "4598   1   0   1   1   0   0   0   0   0   0  ...   1   1   0   0   0   0   0   \n",
       "4599   1   0   0   1   1   0   0   0   0   0  ...   0   0   0   0   0   1   1   \n",
       "4600   0   0   1   1   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "      55  56  57  \n",
       "0      1   1   1  \n",
       "1      1   1   1  \n",
       "2      1   1   1  \n",
       "3      0   0   1  \n",
       "4      0   0   1  \n",
       "...   ..  ..  ..  \n",
       "4596   0   0   0  \n",
       "4597   0   0   0  \n",
       "4598   0   0   0  \n",
       "4599   0   0   0  \n",
       "4600   0   0   0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = feature_prep(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "4596    0\n",
       "4597    0\n",
       "4598    0\n",
       "4599    0\n",
       "4600    0\n",
       "Name: 57, Length: 4601, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=dataset[57]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1ec112c38f6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_IG\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m57\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "feature_IG=[best_feature(feature,dataset,57) for feature in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature(feature,data,lable_col=57):\n",
    "    \n",
    "    labels=data[lable_col]\n",
    "    grop_lab1=labels[data[feature]==0]\n",
    "    grop_lab2=labels[data[feature]==1]\n",
    "\n",
    "    entropy1=entropy(grop_lab1)\n",
    "    entropy2=entropy(grop_lab2)\n",
    "\n",
    "    total_entropy=grop_lab1.size/labels.size*entropy1+grop_lab2.size/labels.size*entropy2\n",
    "    current_entropy=entropy(labels)\n",
    "    feature_IG=current_entropy-total_entropy\n",
    "    \n",
    "    return feature_IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Decision tree classifer \n",
    "# reference: http://www.cse.unsw.edu.au/~billw/cs9414/notes/ml/06prop/id3/id3.html\n",
    "# https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/\n",
    "\n",
    "def DCT(data,originaldata,features,lable_col=57,parent_node_class=None):\n",
    "    labels=data[57]\n",
    "    if len(labels.unique())<=1:\n",
    "#         print(\"Same classification\")\n",
    "        return labels.mode()\n",
    "    \n",
    "    elif data.size==0:\n",
    "        return originaldata[57].mode()\n",
    "    \n",
    "    elif len(features) ==0:  \n",
    "        return parent_node_class \n",
    "    else:      \n",
    "        \n",
    "        \n",
    "       # The \"group number\" of parent node\n",
    "        parent_node_class=data[57].mode()\n",
    "        \n",
    "        # select best feature to split the dataset by calculating the information gain\n",
    "        \n",
    "        \n",
    "        feature_IG=pd.Series([best_feature(feature,data,lable_col) for feature in features])\n",
    "        best=features[feature_IG.idxmax()]\n",
    "        tree = {best:{}} # create tree structure and the root is the best feature gained from the first run\n",
    "\n",
    "        # remove the best feature from feature space\n",
    "        features_cat_update=features.drop(best)\n",
    "        \n",
    "        # split the data by feature values\n",
    "        \n",
    "        for value in data[best].unique():\n",
    "            sub_data=data[data[best]==value].dropna()\n",
    "            subtree = DCT(sub_data,dataset,features_cat_update,lable_col,parent_node_class)\n",
    "\n",
    "            tree[best][value] = subtree \n",
    "        \n",
    "        return tree      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query,tree,default = 1):  \n",
    "    \n",
    "    for key in list(query.keys()):  \n",
    "        if key in list(tree.keys()):  \n",
    "            \n",
    "            try:  \n",
    "                result = tree[key][query[key]]   \n",
    "            except:  \n",
    "                return default  \n",
    "    \n",
    "            result = tree[key][query[key]]  \n",
    "            \n",
    "            if isinstance(result,dict):  \n",
    "                return predict(query,result)  \n",
    "            else:  \n",
    "                return result  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def test(data,tree):  \n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and   \n",
    "    #convert it to a dictionary  \n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")  \n",
    "      \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored  \n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"])   \n",
    "      \n",
    "    #Calculate the prediction accuracy  \n",
    "    for i in range(len(data)):  \n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0)   \n",
    "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[57])/len(data))*100,'%')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=feature_prep(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3067, 58)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the data\n",
    "np.random.seed(0)\n",
    "# np.random.shuffle(dataset)\n",
    "train_set,test_set = train_test_split(dataset,test_size = 1/3)\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.numeric.Int64Index"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['x'+str(i) for i in range(57)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible indexer with Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-9522b832ff1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDCT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-83bd9cfe7ff5>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(data, tree)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#Calculate the prediction accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"predicted\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The prediction accuracy is: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"predicted\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m57\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    581\u001b[0m                 \u001b[1;31m# setting for extensionarrays that store dicts. Need to decide\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[1;31m# if it's worth supporting that.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_align_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_align_series\u001b[1;34m(self, indexer, ser, multiindex_indexer)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Incompatible indexer with Series\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_align_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible indexer with Series"
     ]
    }
   ],
   "source": [
    "tree = DCT(train_set,train_set,train_set.columns[:-1])  \n",
    "test(test_set,tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify testing samples and compute statistics using the testing data results\n",
    "TP=0\n",
    "FN=0\n",
    "FP=0\n",
    "TN=0\n",
    "for ii,y_t in enumerate(y_test):   \n",
    "    x_t = x_test_S[ii] \n",
    "    \n",
    "    # calaulate the probability of testing samples and chosse labels\n",
    "    P_spam = np.log(len(x_train_S_spam)/len(x_train_S))\n",
    "    P_no_spam = np.log(len(x_train_S_no_spam)/len(x_train_S))\n",
    "    for k,x_t_k in enumerate(x_t):\n",
    "        P_spam += np.log(1/std_spam[k]) + (-((x_t_k-mean_spam[k])/std_spam[k])**2/2)\n",
    "        P_no_spam += np.log(1/std_no_spam[k])+ (-((x_t_k-mean_no_spam[k])/std_no_spam[k])**2/2)\n",
    "    \n",
    "#     print(P_spam)\n",
    "#     print(P_no_spam)\n",
    "    # choose the lable based on higher probability of each class\n",
    "    if P_spam>=P_no_spam:  \n",
    "        y_p=1\n",
    "    else:\n",
    "        y_p=0\n",
    "    \n",
    "    # Compate the statistics\n",
    "    if y_t==1 and y_p==1:\n",
    "        TP +=1\n",
    "    elif y_t==1 and y_p==0:\n",
    "        FN +=1\n",
    "    elif y_t==0 and y_p==1:\n",
    "        FP +=1\n",
    "    else:\n",
    "        TN +=1\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F_measure = 2*precision*recall/(precision+recall)\n",
    "accuracy = (TP+TN)/len(y_test)\n",
    "print(\"The precision is: \"+str(precision))\n",
    "print(\"The recall is: \"+str(recall))\n",
    "print(\"The F_measure is: \"+str(F_measure))\n",
    "print(\"The accuracy is: \"+str(accuracy))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
